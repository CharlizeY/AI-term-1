{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "tutorial_3_solution_local.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VG1q0h4MuOvp",
        "B7Tox87FUXwD",
        "NKHIOPm_UwK4",
        "6lBcIUZ1x6Bg",
        "0Bk-PIDvy3-o",
        "uLriyAW593AR"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt7Z9F2urDvb"
      },
      "source": [
        "# Instaling in Google Colab the libraries used for this assignemnt\n",
        "# You do NOT need to understand it to work on this lab assessment\n",
        "\n",
        "# WARNING: if you don't use this Notebook in Google Colab, this block might print some warnings (do not mind them)\n",
        "from IPython.display import HTML, clear_output\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install colabgymrender==1.0.2\n",
        "!wget http://www.atarimania.com/roms/Roms.rar\n",
        "!mkdir /content/ROM/\n",
        "!unrar e /content/Roms.rar /content/ROM/\n",
        "!python -m atari_py.import_roms /content/ROM/\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "20IyxDzgp3tU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "888dbc01-09ac-433f-b429-33f4ab30d5aa"
      },
      "source": [
        "# Importing the libraries\n",
        "\n",
        "import gym\n",
        "from gym.wrappers import Monitor # Allow to record videos\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # Graphical library\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Configuring Pytorch\n",
        "\n",
        "\n",
        "# WARNING: if you don't use this Notebook in Google Colab, comment out these two imports\n",
        "#from colabgymrender.recorder import Recorder # Allow to record videos in Google Colab\n",
        "#Recorder(gym.make(\"CartPole-v0\"), './video') # Defining the video recorder\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2703360/45929032 bytes (5.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5931008/45929032 bytes (12.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b9150464/45929032 bytes (19.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12132352/45929032 bytes (26.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15155200/45929032 bytes (33.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18399232/45929032 bytes (40.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21389312/45929032 bytes (46.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24600576/45929032 bytes (53.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27926528/45929032 bytes (60.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31072256/45929032 bytes (67.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b34471936/45929032 bytes (75.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b37535744/45929032 bytes (81.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40828928/45929032 bytes (88.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b43868160/45929032 bytes (95.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<colabgymrender.recorder.Recorder at 0x7f54ad259d10>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNPHHkRcUTLg"
      },
      "source": [
        "# Solution of Lab Assignment 3 :  \n",
        "See pdf for instructions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q06EldpwfoEB"
      },
      "source": [
        "## Part 1: Introduction to Gym environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl3dm2KOPrfi"
      },
      "source": [
        "### Question 1: Creating the Cartpole environment and performing an episode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzdwlOmsfydj"
      },
      "source": [
        "# Creating the environment and a recorder to save a video in the './random_episode' folder\n",
        "# To save multiple videos, save each mp4 fle to a new directory\n",
        "env = Monitor(gym.make('CartPole-v0'), './random_episode', force=True)\n",
        "\n",
        "# Performing an episode in the environemnt with random actions\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "\n",
        "    action = env.action_space.sample()  # sample a random possible action from the CartPole env\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    state = next_state\n",
        "    \n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG1q0h4MuOvp"
      },
      "source": [
        "### Question 2: Implementing a simple hand-designed policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o3d_CI_Pon1"
      },
      "source": [
        "def simple_policy(state, p_random):\n",
        "    \"\"\"\n",
        "    Simple hand-crafted policy to act in the Cartpole environment.\n",
        "    Input: \n",
        "        - state {tensor} - current state of the environment\n",
        "        - p_random {float} - probability that the action is random\n",
        "    Output: action {int} - action to perform in the environemnt\n",
        "    \"\"\"\n",
        "\n",
        "    if np.random.random() < p_random:\n",
        "        return np.random.randint(2)\n",
        "    \n",
        "    else:\n",
        "        return int(state[2] > 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys68tHGEPh7u"
      },
      "source": [
        "# Rate of random action sampling\n",
        "p_random = 0.2\n",
        "\n",
        "# Performing an episode in the environemnt with simple policy\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    action = simple_policy(state, p_random)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    state = next_state\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjtZS1qHUzHH"
      },
      "source": [
        "## Part 2: Introduction to PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7Tox87FUXwD"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GmBYBmriPSZ"
      },
      "source": [
        "# Graphical class: this class modifies the original Gym class to be able to visualise your prediction\n",
        "# You DO NOT need to understand it to work on this lab assessment\n",
        "\n",
        "from gym.envs.classic_control.cartpole import CartPoleEnv\n",
        "from gym.wrappers.time_limit import TimeLimit\n",
        "\n",
        "class ShowCartPolePredictions(CartPoleEnv):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def step(self, state):\n",
        "        \"\"\"\n",
        "        Step takes the next state as input instead of action.\n",
        "        \"\"\"\n",
        "        self.state = state\n",
        "        x, x_dot, theta, theta_dot = state\n",
        "\n",
        "        done = bool(\n",
        "            x < -self.x_threshold\n",
        "            or x > self.x_threshold\n",
        "            or theta < -self.theta_threshold_radians\n",
        "            or theta > self.theta_threshold_radians\n",
        "                )\n",
        "\n",
        "        reward = 1.\n",
        "            \n",
        "        return np.array(self.state, dtype=np.float32), reward, done, {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKHIOPm_UwK4"
      },
      "source": [
        "### Question 3: Understanding the MLP class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrrOYjZfcEIA"
      },
      "source": [
        "# Multi Layer perceptron class\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, num_hidden, hidden_size):\n",
        "        \"\"\"\n",
        "        Initialise the network.\n",
        "        Input:\n",
        "            - input_size {int} - size of input to the network\n",
        "            - output_size {int} - size of output to the network\n",
        "            - num_hidden {int} - number of hidden layers\n",
        "            - hidden_size {int} - size of each hidden layer\n",
        "        \"\"\"\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_size, hidden_size) # First tranformation from the network input to the input of first hidden layer\n",
        "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_hidden-1)]) # All the hidden transformation\n",
        "        self.output_layer = nn.Linear(hidden_size, output_size) # Last tranformation from the last hidden layer output to the network output\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Get the output of the MLP.\n",
        "        Input: x {tensor} - one element or a batch of element\n",
        "        Ouput: y {tensor} - corresponding output\n",
        "        \"\"\"\n",
        "        x.to(device)\n",
        "        x = self.input_layer(x) # Passing through the input layer\n",
        "        x = F.relu(x) # Applying Relu activation\n",
        "        for layer in self.hidden_layers:\n",
        "          x = layer(x) # Passing through each hidden layer\n",
        "          x = F.relu(x) # Applying Relu activation\n",
        "        x = self.output_layer(x) # Passing through the output layer\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVEqgNkRwgCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bad2e254-e7ac-4893-fb22-ffdc1ae20b51"
      },
      "source": [
        "# Initialise an MLP instance\n",
        "input_size = 10\n",
        "output_size = 10\n",
        "num_hidden = 3\n",
        "hidden_size = 15\n",
        "\n",
        "model = MLP(input_size, output_size, num_hidden, hidden_size)\n",
        "\n",
        "# Creating some false input\n",
        "x = torch.rand(10) # Random tensor\n",
        "print(\"The input is:\\n\", x)\n",
        "\n",
        "# Passing it through the network\n",
        "y = model.forward(x)\n",
        "print(\"\\nThe correpsonding output is:\\n\", y)\n",
        "print(\"\\nThe network has not been trained yet so this output is random.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input is:\n",
            " tensor([0.3314, 0.3177, 0.8721, 0.1225, 0.2413, 0.0461, 0.7277, 0.6055, 0.1542,\n",
            "        0.0031])\n",
            "\n",
            "The correpsonding output is:\n",
            " tensor([ 0.2378,  0.0330,  0.0333,  0.0519, -0.2315,  0.2711, -0.0403, -0.2124,\n",
            "         0.1472, -0.2183], grad_fn=<AddBackward0>)\n",
            "\n",
            "The network has not been trained yet so this output is random.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lBcIUZ1x6Bg"
      },
      "source": [
        "### Question 4: Collecting data to train the state-predictor model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aydDmDSz3olA"
      },
      "source": [
        "def batch_data(state_list, action_list, next_state_list, batch_size, num_batches):\n",
        "  \"\"\"\n",
        "  Reshape the data to match the model requirements.\n",
        "  Input:\n",
        "    - state_list {list of torch.tensor} - list of state encountered during all num_episode episodes\n",
        "    - action_list {list of torch.tensor} - list of action applied during all num_episode episodes\n",
        "    - next_state_list {list of torch.tensor} - list of next state each action lead to during all num_episode episodes\n",
        "    - batch_size {int} - number of steps in a batch\n",
        "    - num_batches {int} - total number of batches\n",
        "  Ouput:\n",
        "    - batched_state_action {torch.tensor} - input of the model of size (batch_size, 5)\n",
        "    - batched_next_state {torch.tensor} - target output of the model of size (batch_size, 4)\n",
        "  \"\"\"\n",
        "  # Reshape and concatenate the state and action (input of the network)\n",
        "  state_action_list = [torch.cat((torch.tensor(state_list[i]).float().unsqueeze(0), torch.tensor(action_list[i]).unsqueeze(0).unsqueeze(0)), dim=-1) for i in range(len(state_list))]\n",
        "  state_action = torch.cat(state_action_list)\n",
        "\n",
        "  # Reshape the next state\n",
        "  next_state = torch.cat([torch.tensor(next_state_list[i]).float().unsqueeze(0) for i in range(len(next_state_list))])\n",
        "\n",
        "  # Rearrange the data into batches\n",
        "  batched_state_action = [state_action[batch*batch_size:(batch+1)*batch_size] for batch in range(num_batches)]\n",
        "  batched_next_state = [next_state[batch*batch_size:(batch+1)*batch_size] for batch in range(num_batches)]\n",
        "\n",
        "  return batched_state_action, batched_next_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKfJjzV6UrfO"
      },
      "source": [
        "def collect_data(num_episodes, p_random): \n",
        "  \"\"\"\n",
        "  Collect the data to train the predictor model.\n",
        "  Input:\n",
        "    - num_episode {int} - number of episodes to collect\n",
        "    - p_random {float} - probability used for the simple policy\n",
        "  Output:\n",
        "    - state_list {list of torch.tensor} - list of state encountered during all num_episode episodes\n",
        "    - action_list {list of torch.tensor} - list of action applied during all num_episode episodes\n",
        "    - next_state_list {list of torch.tensor} - list of next state each action lead to during all num_episode episodes\n",
        "  \"\"\"\n",
        "\n",
        "  # Containers for the data\n",
        "  state_list = [] # List of current states\n",
        "  action_list = [] # List of current actions\n",
        "  next_state_list = [] # List of next step states\n",
        "\n",
        "  # Creating the environment\n",
        "  env = gym.make('CartPole-v0')\n",
        "\n",
        "  for i_episode in range(num_episodes):\n",
        "\n",
        "    if i_episode % 1000 == 0:\n",
        "      print(\"Collecting episode\", i_episode, \"of\", num_episodes)\n",
        "\n",
        "    # Performing the episode\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = simple_policy(state, p_random)\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "      # Fill in the data\n",
        "\n",
        "      state_list.append(state)\n",
        "      action_list.append(action)\n",
        "      next_state_list.append(next_state)\n",
        "\n",
        "      state = next_state\n",
        "\n",
        "  # Closing the environment\n",
        "  env.close()\n",
        "\n",
        "  return state_list, action_list, next_state_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnZ5-DHlyDnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d465e657-6c73-420d-9dc3-7f0a2e542992"
      },
      "source": [
        "# Define parameters for the model\n",
        "num_episodes = 5000 # Total number of episodes collected in our dataset\n",
        "batch_size = 128 # Size of the batch to train the DNN\n",
        "p_random = 0.2 # Parameter of the simple_policy\n",
        "\n",
        "# Collect the data\n",
        "state_list, action_list, next_state_list = collect_data(num_episodes, p_random)\n",
        "num_batches = int(len(state_list)/batch_size)\n",
        "\n",
        "# Reshape them to match the model input/output\n",
        "batched_state_action, batched_next_state = batch_data(state_list, action_list, next_state_list, batch_size, num_batches)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting episode 0 of 5000\n",
            "Collecting episode 1000 of 5000\n",
            "Collecting episode 2000 of 5000\n",
            "Collecting episode 3000 of 5000\n",
            "Collecting episode 4000 of 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bk-PIDvy3-o"
      },
      "source": [
        "### Question 5: Training a state predictor model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-7uYOAw6_3c"
      },
      "source": [
        "def MSE_loss(prediction, target):\n",
        "  \"\"\" \n",
        "  MSE loss function.\n",
        "  Input:\n",
        "    - prediction {torch.tensor} - target\n",
        "    - target {torch.tensor} - model prediction\n",
        "  Output: loss {float} - MSE error between the prediction and the target\n",
        "  \"\"\"\n",
        "  return ((prediction - target)**2).sum(dim=-1).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "scrolled": false,
        "id": "eyeJfvwXp3ta",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "92c21a20-0800-47a2-dee1-7c306fed59d4"
      },
      "source": [
        "# Creating the environment\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# Defining the parameters\n",
        "state_dim = 4\n",
        "action_dim = 1\n",
        "\n",
        "input_size = state_dim + action_dim\n",
        "output_size = state_dim\n",
        "num_hidden = 2\n",
        "hidden_size = 50\n",
        "\n",
        "# Creating the predictor model\n",
        "state_predictor = MLP(input_size, output_size, num_hidden, hidden_size)\n",
        "\n",
        "# Creating the optmiizer\n",
        "optimiser = optim.Adam(state_predictor.parameters())\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "losses = []\n",
        "for epochs in range(num_epochs):\n",
        "    print(\"Epoch:\", epochs, \"of\", num_epochs)\n",
        "    for batch_idx in range(num_batches):\n",
        "\n",
        "        inputs = batched_state_action[batch_idx] # Input of the model for this batch\n",
        "        targets = batched_next_state[batch_idx] # Target output of the model for this batch\n",
        "\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        loss = MSE_loss(state_predictor(inputs), targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "# Closing the environment\n",
        "env.close()\n",
        "\n",
        "# Plot the loss across training\n",
        "plt.plot(losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 of 5\n",
            "Epoch: 1 of 5\n",
            "Epoch: 2 of 5\n",
            "Epoch: 3 of 5\n",
            "Epoch: 4 of 5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f549fad7450>]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sK7wN8a9GKC"
      },
      "source": [
        "# Displaying the learned model dynamics in the CartPole environment\n",
        "\n",
        "# simulated_env allows us to visualise the learned model dynamics\n",
        "# by calling simulated_env.set_next_state(next_state) we set the expected\n",
        "# next_state and we can visualise what the learned dynamics looks like\n",
        "# video saved in the 'learned_dynamics' folder\n",
        "simulated_env = TimeLimit(ShowCartPolePredictions(), max_episode_steps=500)\n",
        "simulated_env = Monitor(simulated_env, './learned_dynamics', force=True)\n",
        "\n",
        "#Performing the episode\n",
        "state = simulated_env.reset()\n",
        "done = False\n",
        "state = torch.tensor(simulated_env.state).float()\n",
        "\n",
        "while not done:\n",
        "\n",
        "    # Predict the state with the model\n",
        "    action = torch.tensor([simple_policy(state, p_random)])\n",
        "    state_action = torch.cat((state, action))\n",
        "    with torch.no_grad():\n",
        "        predicted_state = state_predictor(state_action)\n",
        "        predicted_state = list([float(s) for s in predicted_state.squeeze()])\n",
        "\n",
        "    # Apply it in the environment\n",
        "    state, reward, done, info = simulated_env.step(predicted_state)\n",
        "    state = torch.tensor(simulated_env.state).float()\n",
        "\n",
        "simulated_env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLriyAW593AR"
      },
      "source": [
        "### Question 6: Trying multiple loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbHSF0Bw934N"
      },
      "source": [
        "# Alternative loss function\n",
        "\n",
        "def L1_loss(prediction, target):\n",
        "  \"\"\" \n",
        "  L1 loss function.\n",
        "  Input:\n",
        "    - prediction {torch.tensor} - target\n",
        "    - target {torch.tensor} - model prediction\n",
        "  Output: loss {float} - L1 error between the prediction and the target \n",
        "  \"\"\"\n",
        "  return (abs(prediction - target)).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru5SHJDI-HDB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e371346d-86ea-46a0-fd37-e530483cbfe4"
      },
      "source": [
        "## Torch MSE loss function\n",
        "torch_MSE_loss = nn.MSELoss()\n",
        "\n",
        "# You can call it the same way you would do with our hand design loss:\n",
        "inputs = batched_state_action[0] # Input of the model for this batch\n",
        "targets = batched_next_state[0] # Target output of the model for this batch\n",
        "loss = torch_MSE_loss(state_predictor(inputs), targets)\n",
        "print(loss)\n",
        "\n",
        "## Torch L1 loss function\n",
        "torch_L1_loss = nn.L1Loss()\n",
        "\n",
        "# You can call it the same way you would do with our hand design loss:\n",
        "inputs = batched_state_action[0] # Input of the model for this batch\n",
        "targets = batched_next_state[0] # Target output of the model for this batch\n",
        "loss = torch_L1_loss(state_predictor(inputs), targets)\n",
        "print(loss)\n",
        "\n",
        "## Torch Huber loss function\n",
        "torch_Huber_loss = nn.HuberLoss()\n",
        "\n",
        "# You can call it the same way you would do with our hand design loss:\n",
        "inputs = batched_state_action[0] # Input of the model for this batch\n",
        "targets = batched_next_state[0] # Target output of the model for this batch\n",
        "loss = torch_Huber_loss(state_predictor(inputs), targets)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(8.5933e-06, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0022, grad_fn=<L1LossBackward>)\n",
            "tensor(4.2966e-06, grad_fn=<HuberLossBackward>)\n"
          ]
        }
      ]
    }
  ]
}